{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPq1C1e/BWCyF2OTqHHGAqt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5421Jg9US5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "753fc6bc-3682-4df1-9678-0473be7b094e"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPILehOTOJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import keras\n",
        "from tqdm import tqdm\n",
        "import pickle as pkl\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "gc.enable()\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, TimeDistributed, Activation, Masking, Input\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, dot, concatenate\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.callbacks import * \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from tensorflow.keras.backend import squeeze\n",
        "from tensorflow.keras. optimizers import RMSprop, Adam\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkE11k7TZWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0e1c7dbd-a311-4305-9b1f-46d8732d3c6f"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17008425042718992300\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2195864329879719364\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jLLekZKG_wD",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq по предыдущему"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-0rG89fHEHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/order_features.csv')\n",
        "order_features['prod_seq'] = order_features['prod_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "order_features['prev_seq'] = order_features['prev_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "order_features['prev_orders_seq'] = order_features['prev_orders_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "\n",
        "train_positive = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/train_positive.csv')\n",
        "train_negative = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/train_negative.csv')\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/id_to_token.pkl', 'rb') as f:\n",
        "  id_to_token = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/token_to_id.pkl', 'rb') as f:\n",
        "  token_to_id = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/val_orders.pkl', 'rb') as f:\n",
        "  val_orders = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/test_orders.pkl', 'rb') as f:\n",
        "  test_orders = pkl.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-TO8KFww-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = 2\n",
        "os.makedirs('/content/drive/My Drive/Colab Notebooks/Diplom/models/seq2seq/{0}'.format(it))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ybwGFS_upzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = order_features[(~order_features['order_id'].isin(val_orders)) & (~order_features['order_id'].isin(test_orders))].reset_index(drop=True)\n",
        "df_val = order_features[order_features['order_id'].isin(val_orders)].reset_index(drop=True)\n",
        "df_test = order_features[order_features['order_id'].isin(test_orders)].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvU7h3yilX3",
        "colab_type": "text"
      },
      "source": [
        "###Создаем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2XoH_D10I-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for el in ['<START>', '<END>']:\n",
        "  id_to_token[len(id_to_token)] = el\n",
        "\n",
        "for i_d, token in id_to_token.items():\n",
        "  if token not in token_to_id:\n",
        "    token_to_id[token] = i_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-9gfJ4pimZk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "d8affb15-6c60-46a1-ab8d-f47d6ff5af59"
      },
      "source": [
        "vocab_size = len(id_to_token)\n",
        "hidden_size = 10\n",
        "\n",
        "encoder_inputs = Input(shape=(None,), name='input_encoder')\n",
        "decoder_inputs = Input(shape=(None,), name='input_decoder')\n",
        "\n",
        "embeddings = Embedding(vocab_size + 1, 13, mask_zero=True, name='embedding')\n",
        "\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "\n",
        "encoder_outputs, state_h, state_c = LSTM(hidden_size, return_sequences=True, return_state=True, name='lstm_encoder')(encoder_embeddings)\n",
        "\n",
        "decoder_outputs, _, _ = LSTM(hidden_size, return_sequences=True, return_state=True, name='lstm_decoder')(decoder_embeddings, initial_state=[state_h, state_c])\n",
        "\n",
        "attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2], name='score_attention')\n",
        "attention = Activation('softmax', name='attention_activation')(attention)\n",
        "\n",
        "context = dot([attention, encoder_outputs], axes=[2,1], name='attention_weighted_dot')\n",
        "decoder_combined_context = concatenate([context, decoder_outputs], name='context')\n",
        "\n",
        "output = TimeDistributed(Dense(hidden_size, activation='tanh'), name='dense_tanh')(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(vocab_size+1, activation='softmax'), name='dense')(output)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.00001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_decoder (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_encoder (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 13)     322673      input_encoder[0][0]              \n",
            "                                                                 input_decoder[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_encoder (LSTM)             [(None, None, 10), ( 960         embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_decoder (LSTM)             [(None, None, 10), ( 960         embedding[1][0]                  \n",
            "                                                                 lstm_encoder[0][1]               \n",
            "                                                                 lstm_encoder[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "score_attention (Dot)           (None, None, None)   0           lstm_decoder[0][0]               \n",
            "                                                                 lstm_encoder[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "attention_activation (Activatio (None, None, None)   0           score_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "attention_weighted_dot (Dot)    (None, None, 10)     0           attention_activation[0][0]       \n",
            "                                                                 lstm_encoder[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "context (Concatenate)           (None, None, 20)     0           attention_weighted_dot[0][0]     \n",
            "                                                                 lstm_decoder[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_tanh (TimeDistributed)    (None, None, 10)     210         context[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (TimeDistributed)         (None, None, 24821)  273031      dense_tanh[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 597,834\n",
            "Trainable params: 597,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAjjHgNViFhC",
        "colab_type": "text"
      },
      "source": [
        "###Делаем батч генератор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tqXadhyiGwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(data, batch_size, vocab_token):\n",
        "  step = 0\n",
        "  while True:\n",
        "    if step * batch_size > len(data):\n",
        "      step = 0\n",
        "\n",
        "    tr_temp = data[step * batch_size:(step+1) * batch_size]\n",
        "\n",
        "    maxlen_encoder = max(list(map(lambda x: len(x[0]), tr_temp)))\n",
        "    maxlen_decoder = max(list(map(lambda x: len(x[1]), tr_temp))) + 2\n",
        "\n",
        "    x_encoder = np.zeros((tr_temp.shape[0], maxlen_encoder), dtype=int)\n",
        "    x_decoder = np.zeros((tr_temp.shape[0], maxlen_decoder), dtype=int)\n",
        "    y_decoder = np.zeros((tr_temp.shape[0], maxlen_decoder, 1), dtype=int)\n",
        "\n",
        "    for i in range(len(tr_temp)):\n",
        "      source = tr_temp[i][0]\n",
        "      target = ['<START>'] + tr_temp[i][1] + ['<END>']\n",
        "      for j in range(len(source)):\n",
        "        x_encoder[i,j] = vocab_token[source[j]] + 1\n",
        "      for k in range(len(target)-1):\n",
        "        x_decoder[i,k] = vocab_token[target[k]] + 1\n",
        "        y_decoder[i,k,0] = vocab_token[target[k+1]] + 1\n",
        "\n",
        "    step += 1\n",
        "\n",
        "    yield [x_encoder, x_decoder], y_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUHPiBckiHEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x, y in batch_generator(df_train[df_train['order_number'] != 1][['prod_seq', 'prev_seq']].values, 64, token_to_id):\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds1Ptajf0rs4",
        "colab_type": "text"
      },
      "source": [
        "###Делаем функцию для семплинга последовательностей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUQHZ-ukimo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_sampling_input = Input(shape=(None,))\n",
        "encoder_sampling_embedding = model.get_layer(name='embedding')(encoder_sampling_input)\n",
        "encoder_sampling_output, encoder_sampling_state_h, encoder_sampling_state_c = model.get_layer(name='lstm_encoder')(encoder_sampling_embedding)\n",
        "encoder_states = [encoder_sampling_state_h, encoder_sampling_state_c]\n",
        "encoder_sampling_model = Model(encoder_sampling_input, encoder_states)\n",
        "\n",
        "decoder_sampling_input = Input(shape=(None,))\n",
        "decoder_sampling_h = Input(shape=(hidden_size,))\n",
        "decoder_sampling_c = Input(shape=(hidden_size,))\n",
        "decoder_sampling_states_init = [decoder_sampling_h, decoder_sampling_c]\n",
        "decoder_sampling_embedding = model.get_layer(name='embedding')(decoder_sampling_input)\n",
        "decoder_sampling_output, decoder_sampling_state_h, decoder_sampling_state_c = model.get_layer(name='lstm_decoder')(decoder_sampling_embedding, initial_state=decoder_sampling_states_init)\n",
        "decoder_sampling_output_states = [decoder_sampling_state_h, decoder_sampling_state_c]\n",
        "output = model.get_layer(name='dense')(decoder_sampling_output)\n",
        "decoder_sampling_model = Model([decoder_sampling_input] + decoder_sampling_states_init, [output] + decoder_sampling_output_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Snv-Q73im26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampling(input_sequence, max_output_seq_length):\n",
        "  \n",
        "  input_sequence = np.reshape(list(map(lambda x: token_to_id[x]+1, input_sequence)), (1, -1))\n",
        "\n",
        "  states = encoder_sampling_model.predict(input_sequence)\n",
        "\n",
        "  decoder_input_token = np.zeros((1, 1), dtype='int')\n",
        "  decoder_input_token[0,0] = token_to_id['<START>'] + 1\n",
        "\n",
        "  target_seq = []\n",
        "\n",
        "  isEnd = False\n",
        "\n",
        "  while not isEnd:\n",
        "    output_probs, state_h, state_c = decoder_sampling_model.predict([decoder_input_token] + states)\n",
        "\n",
        "    states = [state_h, state_c]\n",
        "    token_index = np.argmax(output_probs)\n",
        "    decoder_input_token[0,0] = token_index\n",
        "    target_seq.append(token_index-1)\n",
        "\n",
        "    if id_to_token[token_index-1] == '<END>':\n",
        "      isEnd = True\n",
        "\n",
        "    if len(target_seq) >= max_output_seq_length:\n",
        "      isEnd = True\n",
        "\n",
        "  return list(map(lambda x: id_to_token[x], target_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TdL-nH0iwRD",
        "colab_type": "text"
      },
      "source": [
        "###Обучаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv42NEgb23U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Diplom/models/seq2seq/{0}'.format(it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoOT_uSBi0u_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_learning_rate(epoch, lrate):\n",
        "  return lrate\n",
        "\n",
        "checkpoint = ModelCheckpoint(path + 'seq2seq_epochs:{epoch:03d}_loss:{loss:.3f}_accuracy:{sparse_categorical_accuracy:.3f}_\\\n",
        "valloss:{val_loss:.3f}_valaccuracy:{val_sparse_categorical_accuracy:.3f}.hdf5', \n",
        "                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto', save_freq='epoch')\n",
        "stop = EarlyStopping(monitor='loss', min_delta=0.0001)\n",
        "lrs = LearningRateScheduler(my_learning_rate)\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8SigPUG3S-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(path+'seq2seq_epochs:025_loss:3.035_accuracy:0.095_valloss:2.562_valaccuracy:0.098.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC4KuILy3WSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(batch_generator(df_train[df_train['order_number'] != 1][['prod_seq', 'prev_seq']].values, batch_size, token_to_id), \n",
        "                    steps_per_epoch=df_train.shape[0]//batch_size, epochs=epochs, \n",
        "                    validation_data=batch_generator(df_val[['prod_seq', 'prev_seq']].values, batch_size, token_to_id), \n",
        "                    validation_steps=df_val.shape[0]//batch_size,  verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oCybTY_i-Jt",
        "colab_type": "text"
      },
      "source": [
        "### Проверяем качество на валидации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFVCftjin8tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision_lstm = {}\n",
        "recall_lstm = {}\n",
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = 0\n",
        "  recall_lstm[i] = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "for order_id, prev_seq in df_val[['order_id', 'prev_seq']].values:\n",
        "\n",
        "  for i in range(1, 11):\n",
        "    prod_set_features = sampling(prev_seq, i)\n",
        "    recommended_lstm = list(map(lambda x: id_to_token[x], prod_set_features))\n",
        "    precision_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(recommended_lstm)\n",
        "    recall_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(bought)\n",
        "\n",
        "  total += 1\n",
        "\n",
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = precision_lstm[i]/total\n",
        "  recall_lstm[i] = recall_lstm[i]/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12ccyYybqQuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = precision_lstm[i]/total\n",
        "  recall_lstm[i] = recall_lstm[i]/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjhTosAsjDNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "03bf24ce-79a0-462f-aa24-0eba9ad38dd1"
      },
      "source": [
        "precision_lstm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0.06349206349206349,\n",
              " 2: 0.03571428571428571,\n",
              " 3: 0.03439153439153439,\n",
              " 4: 0.025793650793650792,\n",
              " 5: 0.020634920634920638,\n",
              " 6: 0.017195767195767195,\n",
              " 7: 0.014739229024943307,\n",
              " 8: 0.012896825396825396,\n",
              " 9: 0.011463844797178132,\n",
              " 10: 0.010317460317460319}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlNz3PbajDhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e4c1430b-1f3c-4d2c-f860-b3e5da958015"
      },
      "source": [
        "recall_lstm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0.0052431359017036765,\n",
              " 2: 0.006376922749776239,\n",
              " 3: 0.008619301182630865,\n",
              " 4: 0.008619301182630865,\n",
              " 5: 0.008619301182630865,\n",
              " 6: 0.008619301182630865,\n",
              " 7: 0.008619301182630865,\n",
              " 8: 0.008619301182630865,\n",
              " 9: 0.008619301182630865,\n",
              " 10: 0.008619301182630865}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}