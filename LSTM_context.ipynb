{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_context.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOkqOlp8hSgvu7xzST12wXh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcsxjbvXJVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b4d59df-687c-4e3c-a5cb-91bc60fcb25b"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPILehOTOJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import keras\n",
        "from tqdm import tqdm\n",
        "import pickle as pkl\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "gc.enable()\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, TimeDistributed, Activation, Masking, Input\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, dot, concatenate\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.callbacks import * \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from tensorflow.keras.backend import squeeze\n",
        "from tensorflow.keras. optimizers import RMSprop, Adam\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkE11k7TZWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "99701e00-b4d8-4882-d33f-31522bc2a602"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8727428007604648591\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 14330381416386456654\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrwxc-wpv26",
        "colab_type": "text"
      },
      "source": [
        "##Классификация последовательностей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND_sIGI3v5-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/order_features.csv')\n",
        "order_features['prod_seq'] = order_features['prod_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "order_features['prev_seq'] = order_features['prev_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "order_features['prev_orders_seq'] = order_features['prev_orders_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "\n",
        "train_positive = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/train_positive.csv')\n",
        "train_negative = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/train_negative.csv')\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/id_to_token.pkl', 'rb') as f:\n",
        "  id_to_token = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/token_to_id.pkl', 'rb') as f:\n",
        "  token_to_id = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/val_orders.pkl', 'rb') as f:\n",
        "  val_orders = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/test_orders.pkl', 'rb') as f:\n",
        "  test_orders = pkl.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyIy2hnGXVkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = 1\n",
        "os.makedirs('/content/drive/My Drive/Colab Notebooks/Diplom/models/lstm_context/{0}'.format(it))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lne94_nFYp_x",
        "colab_type": "text"
      },
      "source": [
        "### Собираем выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDPS8uhLwIEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_neg = pd.concat((train_positive, train_negative), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSBZH6nwYpEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = pos_neg[(~pos_neg['order_id'].isin(val_orders)) & (~pos_neg['order_id'].isin(test_orders))].sample(frac=1).drop_duplicates(subset=['uxp_total_bought', 'uxp_reorder_ratio',\n",
        "       'times_lastN', 'u_total_orders', 'u_reordered_ratio', 'p_total_purchases', 'p_reorder_ratio', 'order_dow',\n",
        "        'order_hour_of_day', 'days_since_prior_order'])\n",
        "data_val = pos_neg[pos_neg['order_id'].isin(val_orders)].sample(frac=1)\n",
        "data_test = pos_neg[pos_neg['order_id'].isin(test_orders)].sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyYxNYVYG57j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_train = data_train.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')\n",
        "new_data_val = data_val.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')\n",
        "new_data_test = data_test.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')\n",
        "\n",
        "prods_df = train_positive.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax6KWk2kaVrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_train['set'] = new_data_train['prev_seq'] + new_data_train['product_id'].apply(lambda x: [x])\n",
        "new_data_val['set'] = new_data_val['prev_seq'] + new_data_val['product_id'].apply(lambda x: [x])\n",
        "new_data_test['set'] = new_data_test['prev_seq'] + new_data_test['product_id'].apply(lambda x: [x])\n",
        "\n",
        "new_data_train['lens'] = new_data_train['set'].apply(len)\n",
        "new_data_val['lens'] = new_data_val['prev_seq'].apply(len)\n",
        "new_data_test['lens'] = new_data_test['prev_seq'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwsGUhZ-edTu",
        "colab_type": "text"
      },
      "source": [
        "### Объявляем модель и обучаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv_i4w9mrF-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "6dbdd0ef-cbd1-46d2-d72e-cdc669c2826a"
      },
      "source": [
        "hidden_size = 10\n",
        "\n",
        "inp = Input(shape=(None,))\n",
        "inp_dow = Input(shape=(1,1))\n",
        "inp_hod = Input(shape=(1,1))\n",
        "inp_dspo = Input(shape=(1,1))\n",
        "\n",
        "emb = Embedding(len(id_to_token), output_dim=13, name='embedding')(inp)\n",
        "emb_dow = Embedding(7, output_dim=3, name='embedding_dow')(inp_dow)[:, 0, 0, :]\n",
        "emb_hod = Embedding(24, output_dim=3, name='embedding_hod')(inp_hod)[:, 0, 0, :]\n",
        "emb_dspo = Dense(4, name='embedding_dspo')(inp_hod)[:, 0, :]\n",
        "\n",
        "lstm = LSTM(hidden_size, kernel_initializer='he_uniform', name='lstm_1')(emb)\n",
        "\n",
        "context = concatenate([emb_dow, emb_hod, emb_dspo], name='context')\n",
        "final_vector= concatenate([context, lstm], name='final_vector')\n",
        "\n",
        "output = Dense(hidden_size, activation='tanh', kernel_initializer='he_uniform', name='dense_tanh')(final_vector)\n",
        "\n",
        "out = Dense(2, kernel_initializer='he_uniform', activation='softmax')(output)\n",
        "\n",
        "model = Model([inp, inp_dow, inp_hod, inp_dspo], out)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.00001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           [(None, 1, 1)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           [(None, 1, 1)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_dow (Embedding)       (None, 1, 1, 3)      21          input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_hod (Embedding)       (None, 1, 1, 3)      72          input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_dspo (Dense)          (None, 1, 4)         8           input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_25 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_9 (Te [(None, 3)]          0           embedding_dow[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_10 (T [(None, 3)]          0           embedding_hod[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_11 (T [(None, 4)]          0           embedding_dspo[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 13)     322634      input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "context (Concatenate)           (None, 10)           0           tf_op_layer_strided_slice_9[0][0]\n",
            "                                                                 tf_op_layer_strided_slice_10[0][0\n",
            "                                                                 tf_op_layer_strided_slice_11[0][0\n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 10)           960         embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "final_vector (Concatenate)      (None, 20)           0           context[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_tanh (Dense)              (None, 10)           210         final_vector[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           [(None, 1, 1)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            22          dense_tanh[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 323,927\n",
            "Trainable params: 323,927\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7p92mjM0s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Diplom/models/lstm_context/{0}/'.format(it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr3Wb76jh7Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(path+'lstm_context_lr4_epochs:5_precision:0.371_trainloss:0.686_trainacc:0.546_valloss:0.684_valacc:0.571.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_l2FcftFkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_learning_rate(epoch, lrate):\n",
        "  return lrate\n",
        "\n",
        "checkpoint = ModelCheckpoint(path + '2cl_lstm_10emb_epochs:{epoch:03d}.hdf5', \n",
        "                             monitor='binary_accuracy', verbose=1, save_best_only=True, mode='max', save_freq='epoch')\n",
        "stop = EarlyStopping(monitor='loss', min_delta=0.0001)\n",
        "lrs = LearningRateScheduler(my_learning_rate)\n",
        "\n",
        "callbacks_list = [checkpoint, lrs]\n",
        "\n",
        "metrics = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Lo-HL6rMBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "ep = 10\n",
        "\n",
        "for ep in range(ep, ep+1):\n",
        "\n",
        "  print('\\n')\n",
        "  print('Training_epoch_{0}\\n'.format(ep))\n",
        "\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "\n",
        "  train_steps = 0\n",
        "\n",
        "  for ln, group_l in tqdm(new_data_train.groupby('lens'), position=0):\n",
        "    temp_train = group_l.sample(frac=1,random_state=432)[['order_dow', 'order_hour_of_day', 'days_since_prior_order', 'set', 'label']].values\n",
        "\n",
        "    if batch_size > temp_train.shape[0]:\n",
        "      batch_size = temp_train.shape[0]\n",
        "\n",
        "    X_train_seq = np.zeros((temp_train.shape[0], ln), dtype=int)\n",
        "    X_train_dow = np.zeros((temp_train.shape[0], 1, 1), dtype=int)\n",
        "    X_train_hod = np.zeros((temp_train.shape[0], 1, 1), dtype=int)\n",
        "    X_train_dspo = np.zeros((temp_train.shape[0], 1, 1), dtype=int)\n",
        "    y_train = np.zeros((temp_train.shape[0], 2), dtype=int)\n",
        "\n",
        "    for i in range(temp_train.shape[0]):\n",
        "      dow = temp_train[i, 0]\n",
        "      hod = temp_train[i, 1]\n",
        "      dspo = temp_train[i, 2]\n",
        "      seq = temp_train[i, 3]\n",
        "      label = temp_train[i, -1]\n",
        "      for j in range(ln):\n",
        "        X_train_seq[i,j] = token_to_id[seq[j]]\n",
        "      X_train_dow[i, 0, 0] = dow\n",
        "      X_train_hod[i, 0, 0] = hod\n",
        "      X_train_dspo[i, 0, 0] = dspo\n",
        "      y_train[i, label] = 1\n",
        "    \n",
        "    history = model.fit([X_train_seq, X_train_dow, X_train_hod, X_train_dspo], y_train, batch_size=batch_size, epochs=1, shuffle=False, workers=-1)\n",
        "\n",
        "    train_loss += history.history['loss'][0]\n",
        "    train_acc += history.history['binary_accuracy'][0]\n",
        "    train_steps += 1\n",
        "\n",
        "  train_loss /= train_steps\n",
        "  train_acc /= train_steps\n",
        "\n",
        "  print('\\n')\n",
        "  print('train_loss: ', train_loss)\n",
        "  print('train_acc: ', train_acc)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Validation_epoch_{0}\\n'.format(ep))\n",
        "\n",
        "  val_loss = 0\n",
        "  val_acc = 0\n",
        "\n",
        "  val_steps = 0\n",
        "\n",
        "  for ln, group_l in tqdm(new_data_val.groupby('lens'), position=0):\n",
        "    temp_val = group_l.sample(frac=1,random_state=432)[['order_dow', 'order_hour_of_day', 'days_since_prior_order', 'set', 'label']].values\n",
        "\n",
        "    if batch_size > temp_val.shape[0]:\n",
        "      batch_size = temp_val.shape[0]\n",
        "\n",
        "    X_val_seq = np.zeros((temp_val.shape[0], ln), dtype=int)\n",
        "    X_val_dow = np.zeros((temp_val.shape[0], 1, 1), dtype=int)\n",
        "    X_val_hod = np.zeros((temp_val.shape[0], 1, 1), dtype=int)\n",
        "    X_val_dspo = np.zeros((temp_val.shape[0], 1, 1), dtype=int)\n",
        "    y_val = np.zeros((temp_val.shape[0], 2), dtype=int)\n",
        "\n",
        "    for i in range(temp_val.shape[0]):\n",
        "      dow = temp_val[i, 0]\n",
        "      hod = temp_val[i, 1]\n",
        "      dspo = temp_val[i, 2]\n",
        "      seq = temp_val[i, 3]\n",
        "      label = temp_val[i, -1]\n",
        "      for j in range(ln):\n",
        "        X_val_seq[i,j] = token_to_id[seq[j]]\n",
        "      X_val_dow[i, 0, 0] = dow\n",
        "      X_val_hod[i, 0, 0] = hod\n",
        "      X_val_dspo[i, 0, 0] = dspo\n",
        "      y_val[i, label] = 1\n",
        "\n",
        "    loss, acc = model.evaluate([X_val_seq, X_val_dow, X_val_hod, X_val_dspo], y_val, batch_size=batch_size, workers=-1)\n",
        "\n",
        "    val_loss += loss\n",
        "    val_acc += acc\n",
        "    val_steps += 1\n",
        "\n",
        "  val_loss /= val_steps\n",
        "  val_acc /= val_steps\n",
        "\n",
        "  print('\\n')\n",
        "  print('val_loss: ', val_loss)\n",
        "  print('val_acc: ', val_acc)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Target_metric\\n')\n",
        "\n",
        "  precision_lstm = {}\n",
        "  recall_lstm = {}\n",
        "  for i in range(1, 11):\n",
        "    precision_lstm[i] = 0\n",
        "    recall_lstm[i] = 0\n",
        "\n",
        "  total = 0\n",
        "\n",
        "  for user_id, group_u in tqdm(new_data_val.groupby('user_id'), position=0):\n",
        "\n",
        "    all_prod_set = prods_df[prods_df['user_id']==user_id].drop_duplicates(subset='product_id')\n",
        "    all_prod_set['product_id'] = all_prod_set['product_id'].apply(lambda x: token_to_id[x])\n",
        "    prod_set_features = all_prod_set['product_id'].values\n",
        "\n",
        "    for order_id, group_o in group_u.groupby('order_id'):\n",
        "      prod_seq = list(map(lambda x: token_to_id[x], group_o['prev_seq'].values[0]))\n",
        "      dow, hod, dspo = group_o[['order_dow', 'order_hour_of_day', 'days_since_prior_order']].values[0]\n",
        "      #Купленные товары\n",
        "      bought = np.array(list(set(group_o['product_id'])))\n",
        "\n",
        "      #Ищем товары, которые порекомендует сетка\n",
        "      inp_seq = np.zeros((prod_set_features.shape[0], len(prod_seq) + 1), dtype=int)\n",
        "      inp_dow = np.zeros((prod_set_features.shape[0], 1, 1), dtype=int)\n",
        "      inp_hod = np.zeros((prod_set_features.shape[0], 1, 1), dtype=int)\n",
        "      inp_dspo = np.zeros((prod_set_features.shape[0], 1, 1), dtype=int)\n",
        "\n",
        "      for i in range(prod_set_features.shape[0]):\n",
        "        temp_arr = prod_seq + [prod_set_features[i]]\n",
        "        for j in range(len(prod_seq) + 1):\n",
        "          inp_seq[i, j] = temp_arr[j]\n",
        "        inp_dow[i, 0, 0] = dow\n",
        "        inp_hod[i, 0, 0] = hod\n",
        "        inp_dspo[i, 0, 0] = dspo\n",
        "\n",
        "      preds = model.predict([inp_seq, inp_dow, inp_hod, inp_dspo])[:, 1]\n",
        "      indexes = np.argsort(preds)[::-1]\n",
        "\n",
        "      for i in range(1, 11):\n",
        "        recommended_lstm = list(map(lambda x: id_to_token[x], prod_set_features[indexes[:i]]))\n",
        "        precision_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(recommended_lstm)\n",
        "        recall_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(bought)\n",
        "\n",
        "      total += 1\n",
        "\n",
        "  for i in range(1, 11):\n",
        "    precision_lstm[i] = precision_lstm[i]/total\n",
        "    recall_lstm[i] = recall_lstm[i]/total\n",
        "\n",
        "  model.save(path + 'lstm_context_lr4_epochs:{}_precision:{:.3f}_trainloss:{:.3f}_\\\n",
        "trainacc:{:.3f}_valloss:{:.3f}_valacc:{:.3f}.hdf5'.format(ep, precision_lstm[1], train_loss, train_acc, val_loss, val_acc))\n",
        "  \n",
        "  print('\\n')\n",
        "  print(precision_lstm)\n",
        "  print(recall_lstm)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rpOxsbUAbFk",
        "colab_type": "text"
      },
      "source": [
        "###Проверяем качество на тесте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSps3kYJO2Jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "190e895b-5b7c-4fdb-f252-287252bc4250"
      },
      "source": [
        "precision_lstm = {}\n",
        "recall_lstm = {}\n",
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = 0\n",
        "  recall_lstm[i] = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "for user_id, group_u in tqdm(new_data_test.groupby('user_id'), position=0):\n",
        "  \n",
        "  all_prod_set = prods_df[prods_df['user_id']==user_id].drop_duplicates(subset='product_id')\n",
        "  all_prod_set['product_id'] = all_prod_set['product_id'].apply(lambda x: token_to_id[x])\n",
        "  prod_set_features = all_prod_set['product_id'].values\n",
        "  \n",
        "  for order_id, group_o in group_u.groupby('order_id'):\n",
        "    prod_seq = list(map(lambda x: token_to_id[x], group_o['prev_seq'].values[0]))\n",
        "    #Купленные товары\n",
        "    bought = np.array(list(set(group_o['product_id'])))\n",
        "    \n",
        "    #Ищем товары, которые порекомендует сетка\n",
        "    inp_seq = np.zeros((prod_set_features.shape[0], len(prod_seq) + 1), dtype=int)\n",
        "    \n",
        "    for i in range(prod_set_features.shape[0]):\n",
        "      temp_arr = prod_seq + [prod_set_features[i]]\n",
        "      for j in range(len(prod_seq) + 1):\n",
        "        inp_seq[i, j] = temp_arr[j]\n",
        "        \n",
        "    preds = model.predict(inp_seq)[:, 1]\n",
        "    indexes = np.argsort(preds)[::-1]\n",
        "\n",
        "    for i in range(1, 11):\n",
        "      recommended_lstm = list(map(lambda x: id_to_token[x], prod_set_features[indexes[:i]]))\n",
        "      precision_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(recommended_lstm)\n",
        "      recall_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(bought)\n",
        "\n",
        "    total += 1\n",
        "\n",
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = precision_lstm[i]/total\n",
        "  recall_lstm[i] = recall_lstm[i]/total\n",
        "\n",
        "metrics[it] = [precision_lstm, recall_lstm]\n",
        "  \n",
        "with open(path + 'simple_lstm_metrics.pkl', 'wb') as f:\n",
        "  pkl.dump(metrics, f)\n",
        "\n",
        "print('\\n')\n",
        "print(precision_lstm)\n",
        "print(recall_lstm)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6605/6605 [02:26<00:00, 45.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygjREMfP1vr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}