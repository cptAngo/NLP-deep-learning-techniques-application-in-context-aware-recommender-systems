{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN5i8MEEtU2yHrKGuS0XiGK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcsxjbvXJVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5626482c-f720-49ff-bddd-709b854d3d29"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPILehOTOJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import keras\n",
        "from tqdm import tqdm\n",
        "import pickle as pkl\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "gc.enable()\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, TimeDistributed, Activation, Masking, Input\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, dot, concatenate\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.callbacks import * \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from tensorflow.keras.backend import squeeze\n",
        "from tensorflow.keras. optimizers import RMSprop, Adam\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMkE11k7TZWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "01a7276e-f234-472c-d0f1-1081cb1df53d"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 10590221345816099547\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 4654365438363155553\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrwxc-wpv26",
        "colab_type": "text"
      },
      "source": [
        "##Классификация последовательностей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND_sIGI3v5-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/order_features.csv')\n",
        "order_features['prod_seq'] = order_features['prod_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "order_features['prev_seq'] = order_features['prev_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "order_features['prev_orders_seq'] = order_features['prev_orders_seq'].apply(lambda x: list(np.fromstring(x.strip('[ ]'), dtype=int, sep=', ')))\n",
        "\n",
        "train_positive = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/train_positive.csv')\n",
        "train_negative = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/train_negative.csv')\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/id_to_token.pkl', 'rb') as f:\n",
        "  id_to_token = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/token_to_id.pkl', 'rb') as f:\n",
        "  token_to_id = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/val_orders.pkl', 'rb') as f:\n",
        "  val_orders = pkl.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Diplom/instacart-market-basket-analysis/test_train/test_orders.pkl', 'rb') as f:\n",
        "  test_orders = pkl.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyIy2hnGXVkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = 1\n",
        "#os.makedirs('/content/drive/My Drive/Colab Notebooks/Diplom/models/simple_lstm/{0}'.format(it))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lne94_nFYp_x",
        "colab_type": "text"
      },
      "source": [
        "### Собираем выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDPS8uhLwIEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_neg = pd.concat((train_positive, train_negative), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSBZH6nwYpEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = pos_neg[(~pos_neg['order_id'].isin(val_orders)) & (~pos_neg['order_id'].isin(test_orders))].sample(frac=1).drop_duplicates(subset=['uxp_total_bought', 'uxp_reorder_ratio',\n",
        "       'times_lastN', 'u_total_orders', 'u_reordered_ratio', 'p_total_purchases', 'p_reorder_ratio', 'order_dow',\n",
        "        'order_hour_of_day', 'days_since_prior_order'])\n",
        "data_val = pos_neg[pos_neg['order_id'].isin(val_orders)].sample(frac=1)\n",
        "data_test = pos_neg[pos_neg['order_id'].isin(test_orders)].sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyYxNYVYG57j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_train = data_train.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')\n",
        "new_data_val = data_val.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')\n",
        "new_data_test = data_test.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')\n",
        "\n",
        "prods_df = train_positive.merge(order_features[['order_id', 'prev_seq']], how='inner', on='order_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax6KWk2kaVrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data_train['set'] = new_data_train['prev_seq'] + new_data_train['product_id'].apply(lambda x: [x])\n",
        "new_data_val['set'] = new_data_val['prev_seq'] + new_data_val['product_id'].apply(lambda x: [x])\n",
        "new_data_test['set'] = new_data_test['prev_seq'] + new_data_test['product_id'].apply(lambda x: [x])\n",
        "\n",
        "new_data_train['lens'] = new_data_train['set'].apply(len)\n",
        "new_data_val['lens'] = new_data_val['prev_seq'].apply(len)\n",
        "new_data_test['lens'] = new_data_test['prev_seq'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFxd7THoziEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_val_set = new_data_val[new_data_val['label'] == 1]\n",
        "new_test_set = new_data_test[new_data_test['label'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwsGUhZ-edTu",
        "colab_type": "text"
      },
      "source": [
        "### Объявляем модель и обучаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv_i4w9mrF-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "c5a7ba9b-e524-4443-a952-bbc34ccc691e"
      },
      "source": [
        "hidden_size = 10\n",
        "\n",
        "inp = Input(shape=(None,))\n",
        "\n",
        "emb = Embedding(len(id_to_token), output_dim=13, name='embedding')(inp)\n",
        "\n",
        "lstm = LSTM(hidden_size, kernel_initializer='he_uniform', name='lstm_1')(emb)\n",
        "\n",
        "#target_vec = lstm[:,-1,:]\n",
        "#query = Dense(hidden_size, kernel_initializer='he_uniform', name='query')(target_vec)\n",
        "\n",
        "#attention = dot([query, lstm[:,:-1,:]], axes=[1, 2], name='score_attention')\n",
        "#attention = Activation('softmax', name='attention_activation')(attention)\n",
        "\n",
        "#context = dot([attention, lstm[:,:-1,:]], axes=[1,1], name='context')\n",
        "#decoder_combined_context = concatenate([target_vec, context], name='attention_vectors')\n",
        "\n",
        "output = Dense(hidden_size, activation='tanh', kernel_initializer='he_uniform', name='dense_tanh')(lstm)\n",
        "\n",
        "out = Dense(2, kernel_initializer='he_uniform', activation='softmax')(output)\n",
        "\n",
        "model = Model(inp, out)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.00001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 13)          322634    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10)                960       \n",
            "_________________________________________________________________\n",
            "dense_tanh (Dense)           (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 323,726\n",
            "Trainable params: 323,726\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7p92mjM0s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Diplom/models/simple_lstm/{0}/'.format(it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr3Wb76jh7Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(path+'simple_lstm_lr4_epochs:5_precision:0.250_trainloss:0.684_trainacc:0.554_valloss:0.686_valacc:0.559.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_l2FcftFkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_learning_rate(epoch, lrate):\n",
        "  return lrate\n",
        "\n",
        "checkpoint = ModelCheckpoint(path + '2cl_lstm_10emb_epochs:{epoch:03d}.hdf5', \n",
        "                             monitor='binary_accuracy', verbose=1, save_best_only=True, mode='max', save_freq='epoch')\n",
        "stop = EarlyStopping(monitor='loss', min_delta=0.0001)\n",
        "lrs = LearningRateScheduler(my_learning_rate)\n",
        "\n",
        "callbacks_list = [checkpoint, lrs]\n",
        "\n",
        "metrics = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Lo-HL6rMBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "ep = 10\n",
        "\n",
        "for ep in range(ep, ep+1):\n",
        "\n",
        "  print('\\n')\n",
        "  print('Training_epoch_{0}\\n'.format(ep))\n",
        "\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "\n",
        "  train_steps = 0\n",
        "\n",
        "  for ln, group_l in tqdm(new_data_train.groupby('lens'), position=0):\n",
        "    temp_train = group_l.sample(frac=1,random_state=432)[['set', 'label']].values\n",
        "\n",
        "    if batch_size > temp_train.shape[0]:\n",
        "      batch_size = temp_train.shape[0]\n",
        "\n",
        "    X_train_seq = np.zeros((temp_train.shape[0], ln), dtype=int)\n",
        "    y_train = np.zeros((temp_train.shape[0], 2), dtype=int)\n",
        "\n",
        "    for i in range(temp_train.shape[0]):\n",
        "      seq = temp_train[i, 0]\n",
        "      label = temp_train[i, -1]\n",
        "      for j in range(ln):\n",
        "        X_train_seq[i,j] = token_to_id[seq[j]]\n",
        "      y_train[i, label] = 1\n",
        "    \n",
        "    history = model.fit(X_train_seq, y_train, batch_size=batch_size, epochs=1, shuffle=False, workers=-1)\n",
        "\n",
        "    train_loss += history.history['loss'][0]\n",
        "    train_acc += history.history['binary_accuracy'][0]\n",
        "    train_steps += 1\n",
        "\n",
        "  train_loss /= train_steps\n",
        "  train_acc /= train_steps\n",
        "\n",
        "  print('\\n')\n",
        "  print('train_loss: ', train_loss)\n",
        "  print('train_acc: ', train_acc)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Validation_epoch_{0}\\n'.format(ep))\n",
        "\n",
        "  val_loss = 0\n",
        "  val_acc = 0\n",
        "\n",
        "  val_steps = 0\n",
        "\n",
        "  for ln, group_l in tqdm(new_data_val.groupby('lens'), position=0):\n",
        "    temp_val = group_l.sample(frac=1,random_state=432)[['set', 'label']].values\n",
        "\n",
        "    if batch_size > temp_val.shape[0]:\n",
        "      batch_size = temp_val.shape[0]\n",
        "\n",
        "    X_val_seq = np.zeros((temp_val.shape[0], ln), dtype=int)\n",
        "    y_val = np.zeros((temp_val.shape[0], 2), dtype=int)\n",
        "\n",
        "    for i in range(temp_val.shape[0]):\n",
        "      seq = temp_val[i, 0]\n",
        "      label = temp_val[i, -1]\n",
        "      for j in range(ln):\n",
        "        X_val_seq[i,j] = token_to_id[seq[j]]\n",
        "      y_val[i, label] = 1\n",
        "\n",
        "    loss, acc = model.evaluate(X_val_seq, y_val, batch_size=batch_size, workers=-1)\n",
        "\n",
        "    val_loss += loss\n",
        "    val_acc += acc\n",
        "    val_steps += 1\n",
        "\n",
        "  val_loss /= val_steps\n",
        "  val_acc /= val_steps\n",
        "\n",
        "  print('\\n')\n",
        "  print('val_loss: ', val_loss)\n",
        "  print('val_acc: ', val_acc)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Target_metric\\n')\n",
        "\n",
        "  precision_lstm = {}\n",
        "  recall_lstm = {}\n",
        "  for i in range(1, 11):\n",
        "    precision_lstm[i] = 0\n",
        "    recall_lstm[i] = 0\n",
        "\n",
        "  total = 0\n",
        "\n",
        "  for user_id, group_u in tqdm(new_val_set.groupby('user_id'), position=0):\n",
        "\n",
        "    all_prod_set = prods_df[prods_df['user_id']==user_id].drop_duplicates(subset='product_id')\n",
        "    all_prod_set['product_id'] = all_prod_set['product_id'].apply(lambda x: token_to_id[x])\n",
        "    prod_set_features = all_prod_set['product_id'].values\n",
        "\n",
        "    for order_id, group_o in group_u.groupby('order_id'):\n",
        "      prod_seq = list(map(lambda x: token_to_id[x], group_o['prev_seq'].values[0]))\n",
        "      #Купленные товары\n",
        "      bought = np.array(list(set(group_o['product_id'])))\n",
        "\n",
        "      #Ищем товары, которые порекомендует сетка\n",
        "      inp_seq = np.zeros((prod_set_features.shape[0], len(prod_seq) + 1), dtype=int)\n",
        "\n",
        "      for i in range(prod_set_features.shape[0]):\n",
        "        temp_arr = prod_seq + [prod_set_features[i]]\n",
        "        for j in range(len(prod_seq) + 1):\n",
        "          inp_seq[i, j] = temp_arr[j]\n",
        "\n",
        "      preds = model.predict(inp_seq)[:, 1]\n",
        "      indexes = np.argsort(preds)[::-1]\n",
        "\n",
        "      for i in range(1, 11):\n",
        "        recommended_lstm = list(map(lambda x: id_to_token[x], prod_set_features[indexes[:i]]))\n",
        "        precision_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(recommended_lstm)\n",
        "        recall_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(bought)\n",
        "\n",
        "      total += 1\n",
        "\n",
        "  for i in range(1, 11):\n",
        "    precision_lstm[i] = precision_lstm[i]/total\n",
        "    recall_lstm[i] = recall_lstm[i]/total\n",
        "\n",
        "  model.save(path + 'simple_lstm_lr4_epochs:{}_precision:{:.3f}_trainloss:{:.3f}_\\\n",
        "trainacc:{:.3f}_valloss:{:.3f}_valacc:{:.3f}.hdf5'.format(ep, precision_lstm[1], train_loss, train_acc, val_loss, val_acc))\n",
        "  \n",
        "  print('\\n')\n",
        "  print(precision_lstm)\n",
        "  print(recall_lstm)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rpOxsbUAbFk",
        "colab_type": "text"
      },
      "source": [
        "###Проверяем качество на тесте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSps3kYJO2Jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "190e895b-5b7c-4fdb-f252-287252bc4250"
      },
      "source": [
        "precision_lstm = {}\n",
        "recall_lstm = {}\n",
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = 0\n",
        "  recall_lstm[i] = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "for user_id, group_u in tqdm(new_test_set.groupby('user_id'), position=0):\n",
        "  \n",
        "  all_prod_set = prods_df[prods_df['user_id']==user_id].drop_duplicates(subset='product_id')\n",
        "  all_prod_set['product_id'] = all_prod_set['product_id'].apply(lambda x: token_to_id[x])\n",
        "  prod_set_features = all_prod_set['product_id'].values\n",
        "  \n",
        "  for order_id, group_o in group_u.groupby('order_id'):\n",
        "    prod_seq = list(map(lambda x: token_to_id[x], group_o['prev_seq'].values[0]))\n",
        "    #Купленные товары\n",
        "    bought = np.array(list(set(group_o['product_id'])))\n",
        "    \n",
        "    #Ищем товары, которые порекомендует сетка\n",
        "    inp_seq = np.zeros((prod_set_features.shape[0], len(prod_seq) + 1), dtype=int)\n",
        "    \n",
        "    for i in range(prod_set_features.shape[0]):\n",
        "      temp_arr = prod_seq + [prod_set_features[i]]\n",
        "      for j in range(len(prod_seq) + 1):\n",
        "        inp_seq[i, j] = temp_arr[j]\n",
        "        \n",
        "    preds = model.predict(inp_seq)[:, 1]\n",
        "    indexes = np.argsort(preds)[::-1]\n",
        "\n",
        "    for i in range(1, 11):\n",
        "      recommended_lstm = list(map(lambda x: id_to_token[x], prod_set_features[indexes[:i]]))\n",
        "      precision_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(recommended_lstm)\n",
        "      recall_lstm[i] += np.intersect1d(bought, recommended_lstm).shape[0]/len(bought)\n",
        "\n",
        "    total += 1\n",
        "\n",
        "for i in range(1, 11):\n",
        "  precision_lstm[i] = precision_lstm[i]/total\n",
        "  recall_lstm[i] = recall_lstm[i]/total\n",
        "\n",
        "metrics[it] = [precision_lstm, recall_lstm]\n",
        "  \n",
        "with open(path + 'simple_lstm_metrics.pkl', 'wb') as f:\n",
        "  pkl.dump(metrics, f)\n",
        "\n",
        "print('\\n')\n",
        "print(precision_lstm)\n",
        "print(recall_lstm)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6605/6605 [02:26<00:00, 45.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygjREMfP1vr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}